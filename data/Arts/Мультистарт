Научная сессия МИФИ - 2007. IX Всероссийская научно-техническая конференция "Нейроинформатика-2007": Сборник научных трудов. В 3-х частях. Ч.3. М.: МИФИ, 2007. - С.251-257.

В. А. КОСТЕНКО, А. Е. СМОЛИК
Московский Государственный Университет им. М.В. Ломоносова, 
факультет Вычислительной математики и Кибернетики
kost@cs.msu.su, smolex@list.ru 
 
АЛГОРИТМ МУЛЬТИСТАРТА С ОТСЕЧЕНИЕМ ДЛЯ ОБУЧЕНИЯ НЕЙРОННЫХ СЕТЕЙ ПРЯМОГО РАСПРОСТРАНЕНИЯ

В данной статье рассматривается алгоритм мультистарта с отсечением для решения задачи обучения нейронной сети прямого распространения. Основная идея данного алгоритма состоит в том, чтобы уменьшить вычислительную сложность алгоритма мультистарта путем отсечения неперспективных инициализаций на ранних этапах обучения. В статье приведены результаты численного исследования алгоритма и выявлена зависимость эффективности работы алгоритма от его основных параметров.

1 Введение
В настоящее время существует множество алгоритмов обучения нейронных сетей прямого распространения (НС). Наиболее распространены различные вариации алгоритмов «наибыстрейшего спуска» — такие, как алгоритм обратного распространения ошибки [1]. В  их основе лежит итеративная процедура изменения параметров НС в направлении, противоположном вектору градиента ошибки аппроксимации. Эти алгоритмы являются локально-оптимальными и результат обучения зависит от выбранного начального приближения. Альтернативным подходом является применение для обучения НС алгоритмов глобального поиска. В противоположность локально-оптимальным алгоритмам, алгоритмы глобального поиска, такие, как алгоритм имитации отжига [5] или генетический алгоритм Холланда [6], обладают способностью исследовать все пространство возможных решений. Недостатком таких алгоритмов является их высокая вычислительная сложность и проблема настройки параметров алгоритма. 
Основная идея алгоритма мультистарта с отсечением заключается в проведении нескольких параллельных запусков (стартов) некоторого локально-оптимального алгоритма. При этом на ранних этапах обучения выделяются неперспективные старты, которые исключаются из рассмотрения, и процесс обучения продолжается на более узком наборе стартов. Таким образом удается сократить количество циклов обучения, проводимых на неудачных стартах, и уменьшить время работы алгоритма.

2 Задача обучения нейронной сети
Нейронной сетью (НС)  [2] будем называть тройку , в которой:
G — ориентированный ациклический граф, задающий  топологию сети: его вершины соответствуют нейронам, дуги — межнейронным связям;
Т — множество функций перехода сети;
W — множество векторов весовых коэффициентов сети.
Первые два компонента  тройки   задают архитектуру НС.
Инициализацией  архитектуры НС будем называть НС данной архитектуры после присвоения ей некоторого начального набора весовых коэффициентов. 
Выборкой будем называть конечный набор  пар «аргумент — значение аппроксимируемой функции», где ,, N — число входных нейронов (число аргументов функции), M — число выходных нейронов. Выборку будем называть корректной, если  выполняется .
Задача обучения НС с заданной архитектурой состоит в том, чтобы подобрать значения ее весовых коэффициентов таким образом, чтобы максимально повысить эффективность решения поставленной прикладной задачи. В рамках исследуемого в данной работе частного случая задачи обучения НС (когда НС применяется для аппроксимации таблично заданной непрерывной функции) рассматриваются следующие критерии эффективности решения задачи аппроксимации:
1) точность аппроксимации, которая обеспечивается сетью;
2) время обучения сети.
 Точность аппроксимации, обеспечиваемая НС , измеряется величиной среднеквадратической ошибки аппроксимации , вычисляемой на заданной корректной выборке S из аппроксимируемой функции.
Время обучения НС измеряется количеством элементарных математических операций, необходимых для подстройки весов НС, обеспечивающей заданную точность аппроксимации. В случае алгоритма мультистарта с отсечением, исследуемого в данной статье, процесс обучения может быть разделен на некоторое множество последовательных циклов подстройки весов, называемых циклами обучения. Поскольку общее число математических операций, требуемых для осуществления  каждого из циклов обучения можно считать неизменным, а длительность прочих составляющих процесса обучения пренебрежимо мало, в качестве второго критерия  рассматривается общее число циклов обучения . 
	
3 Алгоритм мультистарта с отсечением
Методом отсечения будем называть , где
A —  число начальных инициализаций, A  N,  A > 1.
C — максимальное число циклов обучения для обучения по одному варианту инициализации, C  N.
D — схема отсечения, задаваемая как конечный набор пар натуральных чисел , удовлетворяющий условиям , где n — число шагов алгоритма,  — число циклов обучения на i-ом шаге— число отсечений на i-ом шаге.
K — критерий отсечения — отображение, по которому для  каждого шага  задается функция R1, где NET — архитектура НС, а S — обучающая выборка.
	Критерием отсечения по значению MSE (KMSE) будем называть критерий, в соответствии с которым для любого   .
	В качестве исходных данных для алгоритма мультистарта с отсечением должны быть заданы:
архитектура НС ;
корректная выборка S аппроксимируемой функции;
локально-оптимальный алгоритм обучения L;
функция распределения начальных инициализаций F.
Набор параметров алгоритма задается методом отсечения , который определяет, как будут отсекаться неперспективные инициализации в процессе обучения. 
Алгоритм мультистарта с отсечением, реализующий метод отсечения M на данном приложении, будем обозначать . Ниже приведено описание работы этого алгоритма.
1. Производится A инициализаций архитектуры НС . Каждый весовой коэффициент выбирается случайным образом в соответствии с функцией распределения F. При этом создается A НС, различающихся только своими наборами весов и имеющих архитектуру . Каждой НС присваивается порядковый номер, соответствующий ее начальной инициализации. Порядковые номера НС не меняются в процессе их дальнейшего обучения.   
2. Делается n шагов алгоритма, на каждом i-ом шаге производится ci циклов обучения всех оставшихся НС на выборке S в соответствии с локально-оптимальным алгоритмом L, после чего удаляются ai НС с наименьшими значениями функции  . При этом обучение производится таким образом, чтобы для любых натуральных чисел p и q последовательное применение p, затем q циклов  обучения было эквивалентно однократному применению  p + q циклов обучения.
3. НС, оставшаяся после n-го шага отсечения, проходит   циклов обучения.
4. Получившуюся НС будем называть результирующей, а ее  весовые коэффициенты и значение среднеквадратичной ошибки аппроксимации на заданной выборке будут выданы как результат работы алгоритма.
Общее количество циклов локально-оптимального алгоритма обучения при этом равно
  (3.1)
где 
 будем называть вычислительной сложностью алгоритма мультистарта с отсечением.
Алгоритмом мультистарта будем называть частный случай алгоритма мультистарта с отсечением с числом отсечений n = 1 и схемой отсечения , в котором в качестве критерия отсечения используется KMSE. Очевидно, что общее количество циклов обучения алгоритма мультистарта равно C·A.
4 Результаты численного исследования алгоритма
Для проведения численного исследования описанного алгоритма использовались непрерывные вещественные функции от двух вещественных переменных, рассматриваемые на множестве [0, 1] x [0, 1]. Для исследования были подобраны функции различного вида, а именно с различным количеством экстремумов, различной «сложностью» и «однородностью» рельефа. Для приведения к общей области определения ко всем функциям было применено линейное преобразование координат: , где , — коэффициент масштабирования, подбираемый отдельно для каждой функции. Ниже приведен перечень выбранных функций:
1. , коэффициент масштабирования 1,5;
2. , коэффициент масштабирования 5;
3. , коэффициент масштабирования 1;
4. , коэффициент масштабирования 3;
5. , коэффициент масштабирования 3;
6. , коэффициент масштабирования 10;
7. , коэффициент масштабирования 6;
8. , коэффициент масштабирования 7;
9. , коэффициент масштабирования 15;
10. , коэффициент масштабирования 10.
Из вышеприведенных функций можно выделить три группы по возрастанию «сложности» их рельефа:
f1, f2 — первый тип (простой рельеф);
f3, f4, f5, f6 — второй тип (более сложный рельеф, присутствуют экстремумы);
f7, f8, f9, f10 — третий тип (многоэкстремальный и неоднородный рельеф).
Параметры алгоритма мультистарта с отсечением были заданы следующие:
архитектура нейронной сети — двуслойный персептрон [3] с 10 нейронами в каждом из внутренних слоев;
количество начальных инициализаций (стартов) А = 30;
циклы обучения проводились локально-оптимальным алгоритмом Левенберга-Марквардта [4].
Эксперименты ставились так, чтобы суммарное число циклов обучения в алгоритме мультистарта с отсечением и в алгоритме мультистарта без отсечения было одинаковым — 1500. Количество стартов также было одинаковым — 30. Схемы отсечения  выбирались с суммарным числом циклов обучения, равным 1500, при этом максимальное число циклов обучения на один старт было равным 65. В алгоритме мультистарта без отсечения число циклов обучения на старт было равным 50. Схемы отсечения сравнивались между собой и со схемой без отсечения по величине получаемой ошибки аппроксимации. 
В таблице 1 представлено уменьшение (в процентах) ошибки аппроксимации схем с различным числом шагов отсечения по сравнению с алгоритмом мультистарта без отсечения. Ошибка алгоритма мультистарта без отсечения принята за 100%.

Таблица 1. Результаты сравнения алгоритмов
Типы функций
Мультистарт
без отсечения
Число шагов отсечения


2
4
Первый тип
100 %
65,17 %
51,09 %
Второй тип
100 %
74,24 %
64,31 %
Третий тип
100 %
57,79 %
51,60 %
Все функции
100 %
65,73 %
55,67 %

Были выявлены некоторые общие свойства схем, показавших высокую точность аппроксимации, а именно:
число циклов обучения, которые проводятся над всеми стартами до первого отсечения, в среднем не менее 25 циклов (на старт), что составляет около 38% от общего числа циклов обучения на один старт;
число циклов обучения, которое тратится на обучение (доводку) результирующей сети после последнего шага отсечения, не превышает 10 циклов, что составляет около 15% общего числа циклов;
на первом шаге отсечения отсекается не менее 5 стартов, что составляет около 17% от общего числа стартов.

Список литературы

  1.
Rumelhart D., Hinton G., Williams R. Learning internal representations by error propagation // In Parallel distributed processing, vol. 1, pp. 318-62. Cambridge, MA: MIT Press, 1986.
  2.
Маркин М.И. Синтез нейронной сети под заданное приложение // Диссертация на соискание ученой степени кандидата физико-математических наук. - М.: МГУ, 2001.
  3.
Терехов А.С. Лекции по теории и приложениям искусственных нейронных сетей, Лаборатория Искусственных Нейронных Сетей НТО-2, ВНИИТФ, Снежинск, 1998.
  4.
Hagan M, Menhaj M. Training feedforward networks with the Marquardt algorithm       // IEEE Transactions on Neural Networks, vol. 5, no. 6, 1994, pp. 989-993.
  5.
M. Locatelli. Simulated Annealing Algorithms for Continious Global Optimization. Journal of Optimization Theory and Applications, pp. 104, 121-133 (2000).
  6.
Kitano H. Empirical studies on the speed of convergence of neural network training using genetic algorithms // Proceedings of the Eighth Nat'l Conf. on AI (AAAI90), pp. 789-805. MIT Press, Cambridge, MA, 1990.
